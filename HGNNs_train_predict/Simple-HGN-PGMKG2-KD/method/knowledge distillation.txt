知识蒸馏实现说明
概述
本项目实现了基于图注意力网络(GAT)的知识蒸馏框架，用于异构图上的链接预测任务。通过Teacher-Student模型架构，实现了模型压缩和性能优化。

知识蒸馏架构
模型设计
Teacher Model: 完整的多层GAT网络，参数量大，性能高
Student Model: 轻量化的GAT网络，参数量约为Teacher的一半
训练流程
第一阶段：Teacher模型训练
目标: 训练一个性能最优的Teacher模型
数据: 使用正样本和负样本进行二分类训练
损失函数: 标准的二元交叉熵损失
优化: 早停机制防止过拟合
第二阶段：Student模型知识蒸馏
目标: 让Student模型学习Teacher的知识
输入: 相同的训练数据
监督信号: Teacher的soft targets + 真实标签的hard targets
核心蒸馏损失函数
损失函数组件说明
Hard Loss (硬损失)

学生模型对真实标签的预测损失
确保学生模型能够完成基本任务
Soft Loss (软损失)

学生模型学习教师模型的概率分布
使用KL散度衡量两个分布的差异
temperature参数控制分布的平滑程度
温度参数 (Temperature)

较高的温度使概率分布更加平滑
帮助学生模型学习教师的"不确定性"知识
默认值：2.0
权重参数 (Alpha)

控制硬损失和软损失的平衡
α = 0.5 表示两者权重相等
可根据具体任务调整
训练过程
Student模型训练循环
关键特性
1. 多关系类型支持
针对每种边类型(关系)独立训练Teacher-Student模型对
支持异构图中的多种关系预测
2. 模型压缩效果
隐藏层维度减半：hidden_dim // 2
网络层数减少：num_layers - 1
显著减少参数量和计算开销
3. 可复现性保证
设置全局随机种子：global_seed = args.run * 1000 + 42
每个epoch使用不同但可复现的种子
确保实验结果的一致性
使用方法
命令行参数
关键参数说明
--temperature: 蒸馏温度，控制软目标的平滑程度
--alpha: 硬损失权重，(1-alpha)为软损失权重
--hidden-dim: Teacher模型隐藏层维度，Student自动减半
--num-layers: Teacher模型层数，Student自动减1
输出结果
模型检查点: 保存在checkpoint/目录

Teacher模型：teacher_checkpoint_{dataset}_{layers}_{edge_type}.pt
Student模型：student_checkpoint_{dataset}_{layers}_{edge_type}.pt
预测结果: 保存在predictions0818/目录

每种关系类型的预测结果独立保存
格式：predictions_{relation_type}_{run}.txt
训练日志: 保存在log/目录

详细记录训练过程和错误信息
优势
模型压缩: Student模型参数量显著减少，推理速度更快
性能保持: 通过知识蒸馏保持较高的预测精度
灵活部署: 轻量化的Student模型更适合资源受限环境
知识传递: 有效传递Teacher模型学到的复杂特征表示
应用场景
异构图链接预测
生物信息学中的关联预测
推荐系统中的物品关联
社交网络分析